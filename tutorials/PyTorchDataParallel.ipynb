{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "aHFAYC4rD_DG"
   },
   "outputs": [],
   "source": [
    "#!pip install -q http://download.pytorch.org/whl/cu75/torch-0.2.0.post3-cp27-cp27mu-manylinux1_x86_64.whl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yj-F150wgahi"
   },
   "source": [
    "# DataParallel\n",
    "> By Sung Kim <hunkim@gmail.com>\n",
    "\n",
    "If you have GPUs, it's very easy to use them in PyTorch. Just you put the model on GPU:\n",
    "```python\n",
    "model.gpu()\n",
    "``` \n",
    "Then, you can copy all your tensors to GPU:\n",
    "```python\n",
    "mytensor = my_tensor.gpu()\n",
    "```\n",
    "Please note that just calling `mytensor.gpu()` won't copy the tensor to GPU. You need to assign it to a new tensor and use the tensor on GPU.\n",
    "\n",
    "Furthermore, it's natural to execute your long-waiting forward, backward propagations on multiple GPUs. Unfortunately, PyTorch won't do that automatically for you. Not yet. (It will just use one GPU for you.) \n",
    "\n",
    "However, running your operations on multiple GPUs is very easy. Just you need to make your model dataparallelable using this.  \n",
    "```python\n",
    "model = nn.DataParallel(model)\n",
    "```\n",
    "\n",
    "That's it. If you want to know more, here we are!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ziBo1ivEH_T"
   },
   "source": [
    "## Imports and parameters\n",
    "Let's import our favorite core PyTorch things and define some parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Lw7aQvXPfcbk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Parameters and DataLoaders\n",
    "input_size = 5\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 30\n",
    "data_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1mJXt_M5EYzo"
   },
   "source": [
    "## Dummy DataSet\n",
    "It's fun to play with dataloader. Let's make a dummy (random) one. Just need to implement the getitem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "1O2m1tkgD666"
   },
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset(input_size, 100),\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TARU9WhEuVa"
   },
   "source": [
    "## Simple Model\n",
    "Then, we need a model to run. For DataParallel demo, let's make a simple one. Just get an input and do a linear operation, and output. However, you can make any model including CNN, RNN or even Capsule Net for `DataParallel`.\n",
    "\n",
    "Inside of the model, we just put a print statement to monitor the size of input and output tensors. Please pay attention to the batch part, rank 0 when they print out something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MZXS6GSYGkom"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    # Our model\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.fc(input)\n",
    "        print(\"  In Model: input size\", input.size(), \n",
    "              \"output size\", output.size())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ADuFCC5Ftnf"
   },
   "source": [
    "## Create Model and DataParallel\n",
    "Here is the core part. First, make a model instance, and check if you have multiple GPUs. (If you don't, I feel sorry for you.) If you have, just wrap our model using `nn.DataParallel`. That's it. I know, it's hard to believe, but that's really it!\n",
    "\n",
    "Then, finally put your model on GPU by `model.gpu()`. It's simple and beautiful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "5Q7H_DpQfa9n"
   },
   "outputs": [],
   "source": [
    "model = Model(input_size, output_size)\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ0Iql5XHE-K"
   },
   "source": [
    "## Fun part\n",
    "Now it's the fun part. Just get data from the dataloader and see the size of input and out tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 155,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1509716467009,
     "user": {
      "displayName": "Sung Kim",
      "photoUrl": "//lh5.googleusercontent.com/-2FvoVmUFc28/AAAAAAAAAAI/AAAAAAADyXc/jchVKEokOxI/s50-c-k-no/photo.jpg",
      "userId": "103160441993602454648"
     },
     "user_tz": -480
    },
    "id": "akYPxFPRGlv5",
    "outputId": "81d5fee8-4c9c-4d42-9d4f-8a1470b9dec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "  In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "  In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "  In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
      "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "for data in rand_loader:\n",
    "    if torch.cuda.is_available():\n",
    "        input_var = Variable(data.cuda())\n",
    "    else:\n",
    "        input_var = Variable(data)\n",
    "\n",
    "    output = model(input_var)\n",
    "    print(\"Outside: input size\", input_var.size(),\n",
    "          \"output_size\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3SrEyU1H0bK"
   },
   "source": [
    "## Didn't you see?\n",
    "Hmm, did you see something working here? It seems just batch 30 input and output 30. The model gets 30 and spits out 30. Nothing special.\n",
    "\n",
    "BUT, Wait! This notebook (or yours) does not have GPUs. If you have GPUs, the execution looks like this, called DataParallel!\n",
    "\n",
    "### 2 GPUs\n",
    "```bash\n",
    "# on 2 GPUs\n",
    "Let's use 2 GPUs!\n",
    "\tIn Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "\tIn Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "\tIn Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "\tIn Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "\tIn Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "\tIn Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "\tIn Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
    "\tIn Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
    "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "```\n",
    "\n",
    "### 3 GPUs\n",
    "If you have 3 GPUs, you will see:\n",
    "```bash\n",
    "Let's use 3 GPUs!\n",
    "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "```\n",
    "\n",
    "### Amazing 8 GPUs\n",
    "If you have 8, it's amazing, and you will see this:\n",
    "```bash\n",
    "Let's use 8 GPUs!\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "\tIn Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "\tIn Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "\tIn Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "\tIn Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "\tIn Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "\tIn Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-F6Qz-JiJZqh"
   },
   "source": [
    "## Summary\n",
    "DataParallel splits your data automatically, and send job orders to multiple models on different GPUs using the data. After each model finishes their job, DataParallel collects and merges the results for you. It's really awesome!\n",
    "\n",
    "For more information, please check out http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html and slides at http://bit.ly/PyTorchZeroAll. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "PyTorchDataParallel",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
